#' @param query.field.value Specifies the value of the queried field. The correct values depend
#' on each field. For instance, the field 'primary_countr.iso3' requires a 3-letter ISO3 code such.
#' Syria, in that case, would be 'syr'.
#' @param add.fields Determine which fields should be returned in the query. The field "date.created" is default and comes in all queries.
#' @param limit The number of results that the query should return. It can be any number between
#' 1 and 1000 or 'all'. If "all" is provided the result will return all the records that match
#' that query in the ReliefWeb database. The default is 10.
#' @param from Limits the query from a certain date. The date has to be in the YYYY-MM-DD format. [Not implemented yet. Will be implemented in future versions.]
#' @param to Limits the query up to a certain date. The date has to be in the YYYY-MM-DD format. [Not implemented yet. Will be implemented in future versions.]
#' @param debug Parameter creater for debugging purposes.
#' @param csv If TRUE will store two CVS files: one with the resulting data.frame from the query and a metadata file.
#'
#' @export
#'
#' @examples
#' # Here we are querying all the latest reposts about Syria using the field 'country'.
#' # syria <- rw.query(entity = 'report', query.field = 'country', query.field.value = 'Syria',
#' # add.fields = c('id', 'title', 'date.created', 'primary_country.iso3')
#' # Here we are using the open text parameter 'text.query' to query for 'Aleppo'.
#' # aleppo <- rw.query(entity = 'report', text.query = 'Aleppo', add.fields = c('id', 'date.created',
#' # 'title'))
#'
#' # The result from both queries is a data.frame.
rw.query <- function(entity = NULL,
limit = NULL,
text.query = NULL,
query.field = NULL,
query.field.value = NULL,
add.fields = NULL,
from = NULL,
to = NULL,
debug = FALSE,
csv = FALSE) {
#### Validation tests. ####
# The validation tests before are useful for helping the user make a
# right query and understand why his query isn't working.
# Install depencency packages if not installed
if ("lubridate" %in% rownames(installed.packages()) == FALSE) install.packages("lubridate")
if ("RCurl" %in% rownames(installed.packages()) == FALSE) install.packages("RCurl")
if ("jsonlite" %in% rownames(installed.packages()) == FALSE) install.packages("jsonlite")
if (is.null(query.field) == TRUE && is.null(text.query) == TRUE) { stop("You have to either provide a `text.query' input or a `query.field` + `query.field.value` input.") }
if (is.null(query.field) == FALSE && is.null(query.field.value) == TRUE) { stop("Please provide a value with a query field.") }
if (length(query.field) > 1) { stop('Please provide only one query field. Run rw.query.fields() if you are in doubt.') }
if (is.null(limit) == FALSE && limit < 0 && tolower(limit) != "all") { stop('Please provide an integer between 1 and 1000 or all.') }
if (is.null(limit) == FALSE && limit > 1000 && tolower(limit) != "all") { stop('Please provide an integer between 1 and 1000 or all.') }  # Increase the upper limit of the function.
if (is.null(limit) == FALSE) { limit <- tolower(limit) }
all <- "all"
#### Building the URL snippets and checking the validity of each parameter. ####
# Here we are building the query.url, param-by-param.
if (is.null(entity) == TRUE) { stop('Please provide an entity. \nAt this point only the `report` \nentity is fully implemented.') }
if (is.null(entity) == FALSE) { entity.url <- paste(entity, "/list", sep = "") }
if (is.null(limit) == TRUE) { limit.url <- paste("?limit=", 10, "&", sep = "")
warning("The default limit for this querier is 10. \nIf you need more please provide a number           using \nthe 'limit' parameter.") }
if (is.null(limit) == FALSE) { limit.url <- paste("?limit=",
ifelse(limit == "all", 1000, limit),
"&", sep = "") }
if (is.null(text.query) == TRUE) { text.query.url <- NULL }
if (is.null(text.query) == FALSE) {
text.query.url <- paste("query[value]=",
text.query,
sep = "")
warning('In this version searching the open text field \nwill override whatever other field you have\nincluded the `query` paramenter. In further \nversions the open text field will allow you to\nfurther refine your search.')
}
if (is.null(query.field) == FALSE) { query.field.url <- paste("query[value]=",
query.field,
":",
query.field.value,
sep = "") }
if (is.null(query.field) == TRUE) { query.field.url <- NULL }
# Function for building the right query when more than one field is provided.
many.fields <- function(qf = NULL) {
if (entity == "country") {
} else { ifelse(all(is.na(match(qf, 'date.created')) == TRUE), qf[length(qf) + 1] <- 'date.created', '') }
# date.created is a default field due to sorting -- unless country.
all.fields.url.list <- list()
for (i in 0:(length(qf) - 1)) {
field.url <- paste("fields[include][",i,"]=", qf[i + 1], sep = "")
all.fields.url.list[i + 1] <- paste("&", field.url, sep = "")
}
all.fields.url <- paste(all.fields.url.list, collapse = "")
return(all.fields.url)
}
if (is.null(add.fields) == FALSE) { add.fields.url <- many.fields(qf = add.fields) }
## From and to paramenters. ##
# if (is.null(from) == TRUE) {}  ## Implement in future versions.
# if (is.null(to) == TRUE) {}  ## Implement in future versions.
## Building URL for aquiring data. ##
api.url <- "http://api.rwlabs.org/v0/"
query.url <- paste(api.url,
entity.url,
limit.url,
text.query.url,
query.field.url,
add.fields.url,
ifelse(entity != "country", "&sort[0]=date.created:desc", ""),
sep = "")
#### Fetching the data. ####
if (debug == TRUE) {
x <- paste("The URL being queried is: ", query.url, sep = "")
warning(x)
}
# Getting the count number for iterations later.
count <- data.frame(fromJSON(getURLContent(query.url)))
#     count <- count$data.total[1]  # hacking querier temporarily for getting all the jobs.
count <- 61000  # latest figure from trends.rwlabs.org
#### Enhancement ####
# Here I am querying the url and getting a base data.frame.
# It would probably be better to implement this process within the iteration bellow.
# Using `jsonlite` to fetch JSON from the URL.
query <- data.frame(fromJSON(getURLContent(query.url)))
# Function to convert the resulting lits into rows in the data.frame
rw.fields <- function(df = NULL) {
for (i in 1:length(add.fields)) {
## Multi Fields ##
# Multi fields not implemented. Will be implemented in future version.
# if (length(df$data.list.fields[i]) > 1) { x <- paste(df$data.list.fields[i], collapse = ",") }
if (length(df$data.list.fields[i]) <= 1) { x <- data.frame(as.list(df$data.list.fields[i]))  }
df <- cbind(df, x)
}
df$data.list.fields <- NULL
return(df)
}
query <- rw.fields(df = query)
# Creating a metadata data.frame.
if (csv == TRUE) {
meta.data <- query[1, 1:7]
write.csv(meta.data, file = paste("data/",
paste(add.fields,
collapse = "-",
"-",
entity,
"-metadata.csv", sep = ""), row.names = FALSE))}
# UI element.
print(paste("Fetching ~", ifelse(is.null(limit) == TRUE, 10, ifelse(identical(limit,all) == TRUE, count, limit)),
" records.", sep = ""))
# Creating iterations to go around the 1000-results limitation.
rw.it <- function(df = NULL) {
if ('created' %in% names(df) == TRUE) { to <- df$created[nrow(df)] }
if ('date' %in% names(df) == TRUE) { to <- df$created[nrow(df)] }
final <- df
# Create progress bar.
limit <- ifelse(limit == "all", 1000, limit)
total <- ceiling(count/limit)
pb <- txtProgressBar(min = 0, max = total, style = 3)
for (i in 2:total) {
setTxtProgressBar(pb, i)  # Updates progress bar.
# Collects the latest date entry collected.
if ('created' %in% names(final) == TRUE) {
to <- format(final$created[nrow(final)], scientific = FALSE)
}
if ('date' %in% names(final) == TRUE) {
to <- format(final$created[nrow(final)], scientific = FALSE)
}
# Creates an URL with the latest date collected.
it.url <- paste(query.url, "&filter[field]=date.created&filter[value][to]=",
#                           ifelse(entity == "country",
#                                  "&filter[field]=date.created&filter[value][to]=", ""),
to, sep = "")
if (debug == TRUE) {
print(paste("This is the it.url", it.url, sep = ""))
print(paste("From iteration number ", i, sep = ""))
}
## Error handling function for each iteration.
tryCatch(x <- data.frame(fromJSON(getURLContent(it.url))),
error = function(e) {
print("There was an error in the URL queried. Skipping ...")
final <- final
},
finally = {
x <- rw.fields(df = x)  # Cleaning fields.
final <- rbind(final, x)
}
)
}
close(pb)
return(final)
}
# Only run iterator if we are fetching "all" entries.
# Note: improve to be more > 1000, but not 'all'.
if (identical(limit,all) == TRUE) { query <- rw.it(df = query) }
#### Cleaning the resulting data. ####
# Transform dates from Epoch to year-month-day.
rw.time <- function(df = NULL) {
df$created <- df$created/1000 # To eliminate the miliseconds.
df$created <- as.Date(as.POSIXct(as.numeric(df$created), origin = "1970-01-01"))
return(df)
}
query <- rw.time(df = query)
# Keeping only the columns of interest.
query <- query[,10:ncol(query)]
if (debug == TRUE) {
before.duplicates <- nrow(query)
}
query <- unique(query)
if (debug == TRUE) {
# UI element
after.duplicates <- nrow(query)
duplicates <- before.duplicates - after.duplicates
print(paste("There were ", duplicates, " duplicates in the query."))
}
#   # Storing the resulting data in a CSV file.
if (csv == TRUE) {
write.csv(query, file = "ReliefWeb-query", "-", entity, ".csv", sep = "")
}
print("Done.")
return(query)
}
all.jobs <- rw.query(entity = "job", limit = 'all', text.query = '', add.fields = c('id', 'title', 'url', 'date.created', 'country.iso3'))
all.jobs <- rw.query(entity = "job", limit = 'all', text.query = '', add.fields = c('id', 'title', 'url', 'date.created'))
View(all.jobs)
nrow(all.jobs)
all.jobs <- rw.query(entity = "job", limit = 'all', text.query = '', add.fields = c('id', 'title', 'url', 'date.created'), debug = TRUE)
nrow(all.jobs)
library(RCurl)
library(Rjson)
library(rjson)
date()
x <- date()
x
class(x)
system.time()
sys.time()
sys.time()
Sys.time()
x <- Sys.time()
class(x)
library(RCurl)
library(rjson)
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list"))
getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list")
/getURL
?getURL
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
user = "dataproject", userpwd="humdata", httpauth = 1L))
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="humdata", httpauth = 1L))
getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="humdata", httpauth = 1L)
getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="humdata")
?getURL
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="dataproject:humdata", httpauth = 1L))
View(x)
x
names(x)
names(x)[2]
names(x)[2][1]
y <- names(x)[2]
names(y)
y
y <- names(x)[3]
names(y)
y
View(y)
y <- x[3]
y
length(y)
names(Y)
names(y)
y[1]
length(y[1])
nrow(y[1])
z <- y[1]
unclass(z)
nrow(unclass(z))
length(unclass(z))
nrow(data.frame(z))
data.frame(z)
data.frame(unclass(z))
z
classa(z)
class(z)
z
names(z)
names(z)[1]
names(z)[1][1]
z[1]
z[1][1]
z[[1]]
length(z[[1]])
z$names
z$name
z[1]$name
z[[1]$name
z[[1]]$name
z[[1]]
length(z[[1]])
z[[1]]$name
names(z[[1]])
data.frame(z[[1]])
XML
library(XML)
theurl <- "http://www.ipea.gov.br/portal/index.php/?option=com_content&view=article&id=21745&Itemid=5"
tables <- readHTMLTable(theurl)
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
tables <- readHTMLTable(theurl)
library(RCurl)
tables <- readHTMLTable(getURL(theurl))
tables <- readHTMLTable(getURL(theurl))
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
n.rows
tables
tables[1]
tables[[11]
tables[[1]]
class(tables)
View(tables)
tables <- readHTMLTable(getURL(theurl))
theurl <- (getURL(theurl)
theurl <- (getURL(theurl))
theurl <- (getURL(theurl))
tables <- readHTMLTable(theurl)
tables
getURL('http://www.ipea.gov.br/portal/index.php?option=com_content&view=article&id=21745&Itemid=5')
getURL('http://www.ipea.gov.br/portal/index.php/?option=com_content&view=article&id=13691&Itemid=5')
getURL('http://www.ipea.gov.br/portal/index.php/?option=com_content&view=article&id=13691')
library(rjson)
library(RCurl)
base_url <- 'https://ds-ec2.scraperwiki.com/ye7nhjd/uq816xboyfzffx6/sql'
table_name <- 'unhcr_real_time'  # table name with the viz data.
sql_query <- paste('?q=select%20*%20from%20', table_name, sep = "")
query_url <- getURL(paste(base_url, sql_query, sep = ""))
getData <- function() {
b <- fromJSON(query_url)
for (i in 1:length(b)) {
c <- b[[i]]
if (i == 1) d <- c
else d <- cbind(d, c)
}
z <- data.frame(t(d))
z
}
data <- getData()
# Leave only the columns of interest
# and store the resulting objects into
# different CSV files.
unhcr_real_time <- data.frame(as.character(data$name),
as.character(data$module_name),
as.character(data$module_type),
as.numeric(data$value),
as.character(data$updated_at))
View(unhcr_real_time)
unhcr_real_time$updated_at <- as.Date(unhcr_real_time$updated_at)
as.Date(unhcr_real_time$updated_at)
View(unhcr_real_time)
names(unhcr_real_time)
names(unhcr_real_time) <- c('Emergency', 'Module_Name', 'Modupe_Type', 'Value', 'Updated_at')
unhcr_real_time$Updated_at <- as.Date(unhcr_real_time$Updated_at)
View(Updated_at)
View(unhcr_real_time)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
temp
sum(x)
x
1 + 2 + 3 + 4 / 4
(1 + 2 + 3 + 4) / 4
(1 + 2 + 3 + 4) / 4
sum(x ^ 2 * p) - sum(x * p) ^ 2
x * y
x * p
y <- x * p
y / length(x)
sum(y) / length(x)
y
sum(y)
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
g
movies
ggplot(movies, aes(votes, rating))
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies) + stat_smooth("loess")
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies) + geom_smooth(method = 'loess')
qplot(votes, rating, data = movies) + geom_smooth()  # works, but loess?
?geom_smooth
qplot(votes, rating, data = movies, panel = panel.loess)  # what?
library(lattice)
xyplot()
bwplot()
?bwplot()
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(RCurl)
library(rjson)
library(sqldf)
library(lubridate)
library(ggplot2)
# Storing the data in a SW database.
db <- dbConnect(SQLite(), dbname="scraperwiki.sqlite")
#     dbWriteTable(db, "data", data, row.names = FALSE, append = TRUE) # for append
dbListFields(db, "data")
NewData <- dbReadTable(db, "data")
dbDisconnect(db)
db <- dbConnect(SQLite(), dbname="scraperwiki.sqlite")
dbListTables(db)
dbDisconnect(db)
db <- dbConnect(SQLite(), dbname="scraperwiki.sqlite")
dbListTables(db)
?dbListTables
dbListTables(db)
NewData <- dbReadTable(db, "data")
NewData <- dbReadTable(db, "value")
setwd("~/Documents/Programming/unhcr_real_time")
indicator <- read.csv('data/indicator.csv')
value <- read.csv('data/value.csv')
names(value)
names(indicator)
data <- merge(value, indicator, by = 'indID', all.x = TRUE)
norw(value)
nrow(value)
nrow(dat)
nrow(data)
View(data)
x <- as.Date(data$period)
summary(x)
View(data)
indicator <- read.csv('data/indicator.csv')
value <- read.csv('data/value.csv')
# denormalizing table
data <- merge(value, indicator, by = 'indID', all.x = TRUE)
x <- as.Date(data$period)
summary(x)
data$period <- as.Date(data$period)
names(data)
x <- as.numeric(data$value)
summary(x)
data$value <- as.numeric(data$value)
names(data)
symmary(data$region)
summary(data$region)
data_c <- data[data$region == 'SYR' | data$region == 'CAR',]
summary(data_c$region)
names(data_c)
summary(data_c$indID)
summary(data_c$name)
# plotting
ggplot(data_c) + theme_bw() +
geom_line(aes(period, value), stat = "identity") +
facet_wrap(~ indID)
library(ggplot2)
# plotting
ggplot(data_c) + theme_bw() +
geom_line(aes(period, value), stat = "identity") +
facet_wrap(~ indID)
# plotting
ggplot(data_c) + theme_bw() +
geom_line(aes(period, value), stat = "identity") +
facet_wrap(~ name)
unique(data$name)
data_c <- data[data$name == 'Refugees from the Central African Republic' | data$name == 'Registered Syrian Refugees', ]
View(data_c)
library(reshape2)
?dcast
View(data)
data_cast <- dcast(data_c, period + value ~ region)
View(data_cast)
View(data_cast)
data_c <- data[data$name == 'Refugees from the Central African Republic', ]
car <- data[data$name == 'Refugees from the Central African Republic', ]
syria <- data[data$name == 'Registered Syrian Refugees', ]
car_c <- dcast(car, period + value ~ region)
syria_c <- dcast(syria, period + value ~ region)
View(car_c)
syria_c <- dcast(syria, period + value ~ region)
View(syria_c)
car_c <- dcast(car, period + value ~ region)
View(car_c)
View(syria_c)
names(car_c)
car_c$CAR <- NULL
names(car_c) <- c('date', 'n_refugees')
names(syria_c) <- c('date', 'n_refugees')
write.csv(car_c, 'http/data/car.csv', row.names = F)
write.csv(syria_c, 'http/data/syria.csv', row.names = F)
syria_c <- dcast(syria, period + value ~ region)
names(syria_c)
syria_c$SYR <- NULL
names(SYR)
names(syria_c)
names(syria_c) <- c('date', 'n_refugees')
write.csv(syria_c, 'http/data/syria.csv', row.names = F)
unique(syria$name)
unique(car$name)
